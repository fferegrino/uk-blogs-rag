{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Statistical\" metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are dealing with applications that generate text and we want to have more flexibility when evaluating the quality of the generated text, we can use \"statistical\" metrics.\n",
    "\n",
    "BLEU and ROUGE are two of the most popular statistical metrics for evaluating the quality of the generated text. \n",
    "\n",
    "These metrics work by comparing the generated text to a \"golden\" reference, their flexibility comes from the fact that it not only checks for specific words (like we did with the rule-based evaluations), but for the entire sequence, and the order of the words, the sentence length, etc.\n",
    "\n",
    "At the core, BLEU and ROUGE decomposes the generated text into **n-grams** (chunks of words) and compare them to the n-grams of the \"golden\" reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, imagine we have the following text:\n",
    "\n",
    "> Policy Lab has experimented with Artificial Intelligence (AI) in policy development with teams across government, and beyond, for a number of years. In 2019 we worked with the Department for Transportâ€™s data science team to consider the role that AI could play in improving the efficiency and effectiveness of the policy consultation process. In 2022 we used AI to create a vision for the future of Hounslow with the local authority. In 2023, we commissioned the creation of the Ecological Intelligence Agency, a speculative artefact to help experience the role AI might have in future decision-making in environmental policy. \n",
    "\n",
    "And we have asked the original writer to provide a summary of the text, **This is our golden reference**:\n",
    "\n",
    "> Policy Lab has explored the use of AI in policy development across various government projects, including improving policy consultation processes, envisioning future urban planning, and investigating AI's potential role in environmental policy decision-making.\n",
    "\n",
    "### What we want to evaluate\n",
    "\n",
    "And we have requested different language models to provide a summary of the text using a variety of prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Reference summary\n",
       "Policy Lab has explored the use of AI in policy development across various government projects, including improving policy consultation processes, envisioning future urban planning, and investigating AI's potential role in environmental policy decision-making.\n",
       "\n",
       "## Model 1\n",
       "Policy Lab has explored the application of Artificial Intelligence in diverse government policy initiatives, including enhancing policy consultation, envisioning local community futures, and examining AI's potential role in environmental policy decisions.\n",
       "\n",
       "## Model 2\n",
       "Policy Lab has explored AI applications in governmental policy creation, collaborating with various agencies to enhance consultation procedures, generate urban forecasts, and envision futuristic environmental decision-making tools.\n",
       "\n",
       "## Model 3\n",
       "Policy Lab has been exploring the application of Artificial Intelligence in various government policy initiatives for several years, including improving policy consultations, envisioning the future of local communities, and speculating on the role of AI in environmental decision-making.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reference_summary = \"Policy Lab has explored the use of AI in policy development across various government projects, including improving policy consultation processes, envisioning future urban planning, and investigating AI's potential role in environmental policy decision-making.\"\n",
    "summary_1 = \"Policy Lab has explored the application of Artificial Intelligence in diverse government policy initiatives, including enhancing policy consultation, envisioning local community futures, and examining AI's potential role in environmental policy decisions.\"\n",
    "summary_2 = \"Policy Lab has explored AI applications in governmental policy creation, collaborating with various agencies to enhance consultation procedures, generate urban forecasts, and envision futuristic environmental decision-making tools.\"\n",
    "summary_3 = \"Policy Lab has been exploring the application of Artificial Intelligence in various government policy initiatives for several years, including improving policy consultations, envisioning the future of local communities, and speculating on the role of AI in environmental decision-making.\"\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## Reference summary\\n{reference_summary}\\n\n",
    "## Model 1\\n{summary_1}\\n\n",
    "## Model 2\\n{summary_2}\\n\n",
    "## Model 3\\n{summary_3}\\n\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## BLEU\n",
    "\n",
    "BLEU stands for Bilingual Evaluation Understudy; and it is a metric that was designed for evaluating the quality of the generated text by comparing it to a set of reference translations.\n",
    "\n",
    "However, it can be used for other applications, such as evaluating the quality of a generated answer from a RAG pipeline, a summarization task, or even the expected response of a chatbot.\n",
    "\n",
    "\n",
    "We can then evaluate the quality of the generated summaries using BLEU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perfect match\n",
    "sentence_bleu(\n",
    "    [reference_summary.lower().split()],\n",
    "    reference_summary.lower().split()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27940187870698063"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu(\n",
    "    [summary_1.lower().split()],\n",
    "    reference_summary.lower().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0925329498915617"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu(\n",
    "    [summary_2.lower().split()],\n",
    "    reference_summary.lower().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonioferegrino/hub/uk-blogs-rag/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.6524797765262606e-78"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu(\n",
    "    [summary_3.lower().split()],\n",
    "    reference_summary.lower().split()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE\n",
    "\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. Just like BLEU, it was designed for evaluating the quality of the generated text by comparing it to a set of reference.\n",
    "\n",
    "In practice ROUGE is actually a set of metrics, each one capturing a different aspect of the generated text, for example:\n",
    "\n",
    "- ROUGE-1: Unigram\n",
    "- ROUGE-2: Bigram\n",
    "- ROUGE-L: Longest Common Subsequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.71875, recall=0.6571428571428571, fmeasure=0.6865671641791045),\n",
       " 'rouge2': Score(precision=0.3870967741935484, recall=0.35294117647058826, fmeasure=0.36923076923076925),\n",
       " 'rougeL': Score(precision=0.6875, recall=0.6285714285714286, fmeasure=0.6567164179104478)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scorer.score(reference_summary, summary_1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.5357142857142857, recall=0.42857142857142855, fmeasure=0.47619047619047616),\n",
       " 'rouge2': Score(precision=0.14814814814814814, recall=0.11764705882352941, fmeasure=0.13114754098360654),\n",
       " 'rougeL': Score(precision=0.5, recall=0.4, fmeasure=0.4444444444444445)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scorer.score(reference_summary, summary_2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.5897435897435898, recall=0.6571428571428571, fmeasure=0.6216216216216216),\n",
       " 'rouge2': Score(precision=0.2894736842105263, recall=0.3235294117647059, fmeasure=0.30555555555555564),\n",
       " 'rougeL': Score(precision=0.5384615384615384, recall=0.6, fmeasure=0.5675675675675675)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scorer.score(reference_summary, summary_3)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
